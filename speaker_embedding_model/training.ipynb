{"cells":[{"cell_type":"markdown","metadata":{},"source":["This Notebook is for example training speaker embedding\n","I used kaggle GPU for training this model. \n","The dataset can be download at https://www.openslr.org/12\n","or access at: https://www.kaggle.com/datasets/hieugiaosu/librispeech"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-28T02:59:28.030373Z","iopub.status.busy":"2024-05-28T02:59:28.029232Z","iopub.status.idle":"2024-05-28T02:59:30.928356Z","shell.execute_reply":"2024-05-28T02:59:30.927166Z","shell.execute_reply.started":"2024-05-28T02:59:28.030325Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import gc\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchaudio\n","from torch.utils.data import DataLoader,Dataset\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import librosa"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T02:59:30.931377Z","iopub.status.busy":"2024-05-28T02:59:30.930670Z","iopub.status.idle":"2024-05-28T02:59:31.044957Z","shell.execute_reply":"2024-05-28T02:59:31.043673Z","shell.execute_reply.started":"2024-05-28T02:59:30.931338Z"},"trusted":true},"outputs":[{"data":{"text/plain":["251"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["len(os.listdir('/kaggle/input/librispeech/train-clean-100/LibriSpeech/train-clean-100'))"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T02:59:31.046922Z","iopub.status.busy":"2024-05-28T02:59:31.046560Z","iopub.status.idle":"2024-05-28T02:59:44.251139Z","shell.execute_reply":"2024-05-28T02:59:44.249985Z","shell.execute_reply.started":"2024-05-28T02:59:31.046894Z"},"trusted":true},"outputs":[],"source":["train_df = pd.DataFrame([],columns=['speaker','audio_file'])\n","addition_data_from_test = [1089,1188,121,1221,1284]\n","test_list = list([])\n","speaker_map = {int(speaker):idx for idx,speaker in enumerate(os.listdir('/kaggle/input/librispeech/train-clean-100/LibriSpeech/train-clean-100'))}\n","for i in addition_data_from_test:\n","    speaker_map[i] = len(speaker_map.keys())\n","for speaker in os.listdir('/kaggle/input/librispeech/train-clean-100/LibriSpeech/train-clean-100'):\n","    audio_file = list([])\n","    for chapter in os.listdir(f'/kaggle/input/librispeech/train-clean-100/LibriSpeech/train-clean-100/{speaker}'):\n","        audio_file_list = list(filter(lambda x: 'txt' not in x,os.listdir(f'/kaggle/input/librispeech/train-clean-100/LibriSpeech/train-clean-100/{speaker}/{chapter}')))\n","        audio_file_list = list(map(lambda x: (speaker_map[int(speaker)],f\"/kaggle/input/librispeech/train-clean-100/LibriSpeech/train-clean-100/{speaker}/{chapter}/{x}\"),audio_file_list))\n","        audio_file = audio_file + audio_file_list\n","    new_rows = pd.DataFrame(audio_file[:-3],columns=['speaker','audio_file'])\n","    train_df = pd.concat([train_df,new_rows],axis=0)\n","    test_list = test_list + audio_file[-3:]\n","for speaker in addition_data_from_test:\n","    audio_file = list([])\n","    for chapter in os.listdir(f'/kaggle/input/librispeech/test-clean/LibriSpeech/test-clean/{speaker}'):\n","        audio_file_list = list(filter(lambda x: 'txt' not in x,os.listdir(f'/kaggle/input/librispeech/test-clean/LibriSpeech/test-clean/{speaker}/{chapter}')))\n","        audio_file_list = list(map(lambda x: (speaker_map[int(speaker)],f\"/kaggle/input/librispeech/test-clean/LibriSpeech/test-clean/{speaker}/{chapter}/{x}\"),audio_file_list))\n","        audio_file = audio_file + audio_file_list\n","    new_rows = pd.DataFrame(audio_file[:-3],columns=['speaker','audio_file'])\n","    train_df = pd.concat([train_df,new_rows],axis=0)\n","    test_list = test_list + audio_file[-3:]\n","#     test_list.append(audio_file[-1])\n","test_df = pd.DataFrame(test_list,columns=['speaker','audio_file'])"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T02:59:44.254296Z","iopub.status.busy":"2024-05-28T02:59:44.253958Z","iopub.status.idle":"2024-05-28T02:59:44.260728Z","shell.execute_reply":"2024-05-28T02:59:44.259546Z","shell.execute_reply.started":"2024-05-28T02:59:44.254267Z"},"trusted":true},"outputs":[],"source":["def preprocess(sample_rate=16000,n_fft=400,hop_length=160,f_min=80,f_max=4000,n_mels=64):\n","    f = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n","                                             n_fft=n_fft,hop_length=hop_length,\n","                                             f_min=f_min,f_max=f_max,\n","                                             n_mels=n_mels)\n","    def proc(audio):\n","        with torch.no_grad():\n","            mel = f(audio)\n","            mel = torch.log(1+mel)\n","            mel = torch.tanh(mel)\n","        return mel\n","    return proc"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T02:59:44.280303Z","iopub.status.busy":"2024-05-28T02:59:44.279973Z","iopub.status.idle":"2024-05-28T02:59:44.291915Z","shell.execute_reply":"2024-05-28T02:59:44.290936Z","shell.execute_reply.started":"2024-05-28T02:59:44.280276Z"},"trusted":true},"outputs":[],"source":["class SpeakerEmbeddingDataset(Dataset):\n","    def __init__(self,data,sample_rate=16000,window_length=0.1,max_chunk_num = 50):\n","        super().__init__()\n","        self.data = data\n","        self.sample_rate = sample_rate\n","        self.window_length = int(window_length*sample_rate)\n","        self.max_chunk_num = max_chunk_num\n","        self.max_audio_length = 16000*5\n","        self.proc = preprocess()\n","        \n","    def __len__(self): return len(self.data)*2\n","    def __getitem__(self,idx):\n","        i = idx//2\n","        r = idx%2\n","        row = self.data.iloc[i]\n","        audio,rate = torchaudio.load(row['audio_file'])\n","        audio = audio.squeeze()\n","        if r==0:\n","            audio = audio[:audio.shape[0]//2]\n","        else:\n","            audio = audio[audio.shape[0]//2:]\n","        if audio.shape[0] < self.max_audio_length:\n","            padding_size = self.max_audio_length - audio.shape[0]\n","            padding = torch.zeros(padding_size).float()\n","            audio = torch.cat([padding,audio],dim = 0)\n","        elif audio.shape[0]> self.max_audio_length:\n","            audio = audio[:self.max_audio_length]\n","        audio = self.proc(audio)\n","        speaker = torch.tensor([int(row['speaker'])])\n","        return audio,speaker"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T02:59:44.293410Z","iopub.status.busy":"2024-05-28T02:59:44.293070Z","iopub.status.idle":"2024-05-28T02:59:44.393563Z","shell.execute_reply":"2024-05-28T02:59:44.392524Z","shell.execute_reply.started":"2024-05-28T02:59:44.293384Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["56092 1536\n"]}],"source":["train_ds = SpeakerEmbeddingDataset(train_df)\n","test_ds = SpeakerEmbeddingDataset(test_df)\n","TESTSIZE =len(test_ds)\n","print(len(train_ds),len(test_ds))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T02:59:44.396048Z","iopub.status.busy":"2024-05-28T02:59:44.395091Z","iopub.status.idle":"2024-05-28T02:59:44.407337Z","shell.execute_reply":"2024-05-28T02:59:44.406328Z","shell.execute_reply.started":"2024-05-28T02:59:44.396010Z"},"trusted":true},"outputs":[],"source":["class SpeakerEmbedding(nn.Module):\n","    def __init__(self,input_dim,hidden_state_dim,embedding_dim):\n","        super().__init__()\n","        self.input_transform = nn.Sequential(\n","            nn.Linear(input_dim,hidden_state_dim),\n","            nn.ELU(),\n","            nn.Linear(hidden_state_dim,hidden_state_dim),\n","            nn.ELU()\n","        )\n","        \n","        self.transformer = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(d_model=2*hidden_state_dim, nhead=2, activation='relu',batch_first=True),\n","            num_layers = 2\n","        )\n","        \n","        self.fc = nn.Linear(2*hidden_state_dim,embedding_dim)\n","       \n","        self.hidden_state_dim = hidden_state_dim\n","        self.embedding_dim = embedding_dim\n","        self.input_dim = input_dim\n","    def forward(self,x):\n","        batch_size = x.shape[0]\n","        seq_len = x.shape[2]\n","        i = torch.transpose(x,1,2)\n","        i = self.input_transform(i)\n","        pos = self.get_sinusoidal_positional_encoding(seq_len,self.hidden_state_dim,i.device).expand(batch_size,-1,-1)\n","        q = torch.cat([i,pos],dim = -1)\n","        att = self.transformer(q)\n","        o = self.fc(att[:,-1,:])\n","        return o\n","        \n","    def get_sinusoidal_positional_encoding(self, max_len, d_model,device=None):\n","        position = torch.arange(max_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n","\n","        pos_encoding = torch.zeros((max_len, d_model))\n","        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n","        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n","        pos_encoding = pos_encoding.to(device)\n","        return pos_encoding\n","        "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T02:59:44.408954Z","iopub.status.busy":"2024-05-28T02:59:44.408613Z","iopub.status.idle":"2024-05-28T02:59:44.423464Z","shell.execute_reply":"2024-05-28T02:59:44.422416Z","shell.execute_reply.started":"2024-05-28T02:59:44.408926Z"},"trusted":true},"outputs":[],"source":["class ArcFace(nn.Module):\n","    \"\"\"\n","This is my reimplement of the ArcFace loss function\n","    \"\"\"\n","    def __init__(self,numClasses,embeddingSize,margin,scale,eps=1e-6):\n","        super().__init__()\n","        self.numClasses = numClasses\n","        self.embeddingSize = embeddingSize\n","        self.m = margin\n","        self.s = scale\n","        self.eps = eps\n","        self.W = nn.Parameter(torch.Tensor(numClasses, embeddingSize))\n","        nn.init.xavier_normal_(self.W)\n","    def forward(self,embeddings,labels=None):\n","        if labels is not None:\n","            batch_size = labels.size(0)\n","            cos = F.linear(F.normalize(embeddings), F.normalize(self.W))\n","            one_hot_encoding_labels = torch.zeros(batch_size, self.numClasses, device=labels.device)\n","            one_hot_encoding_labels.scatter_(1, labels.unsqueeze(-1), 1)\n","            cos_target_classes = cos[one_hot_encoding_labels==1]\n","            theta =  torch.acos(torch.clamp(cos_target_classes, -1 + self.eps, 1 - self.eps))\n","            cos_with_margin = torch.cos(theta+self.m)\n","            diff = (cos_with_margin-cos_target_classes).unsqueeze(1)\n","            logits = cos + one_hot_encoding_labels*diff\n","            logits = self.s*logits\n","            return logits\n","        else:\n","            cos = F.linear(F.normalize(embeddings), F.normalize(self.W))\n","            return cos"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T02:59:44.427727Z","iopub.status.busy":"2024-05-28T02:59:44.427384Z","iopub.status.idle":"2024-05-28T02:59:44.439082Z","shell.execute_reply":"2024-05-28T02:59:44.437992Z","shell.execute_reply.started":"2024-05-28T02:59:44.427701Z"},"trusted":true},"outputs":[],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T02:59:44.440569Z","iopub.status.busy":"2024-05-28T02:59:44.440263Z","iopub.status.idle":"2024-05-28T02:59:44.450332Z","shell.execute_reply":"2024-05-28T02:59:44.449392Z","shell.execute_reply.started":"2024-05-28T02:59:44.440544Z"},"trusted":true},"outputs":[],"source":["class Classifier(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.emb = SpeakerEmbedding(64,128,256)\n","        self.arcFace = ArcFace(256,256,0.4,64)\n","    def forward(self,x,labels=None):\n","        y = self.emb(x)\n","        logits = self.arcFace(y,labels)\n","        return logits"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T02:59:44.451916Z","iopub.status.busy":"2024-05-28T02:59:44.451522Z","iopub.status.idle":"2024-05-28T02:59:44.499472Z","shell.execute_reply":"2024-05-28T02:59:44.498372Z","shell.execute_reply.started":"2024-05-28T02:59:44.451880Z"},"trusted":true},"outputs":[],"source":["model =  Classifier()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T02:59:44.500946Z","iopub.status.busy":"2024-05-28T02:59:44.500619Z","iopub.status.idle":"2024-05-28T02:59:44.507735Z","shell.execute_reply":"2024-05-28T02:59:44.506602Z","shell.execute_reply.started":"2024-05-28T02:59:44.500919Z"},"trusted":true},"outputs":[],"source":["def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n","  fig, axs = plt.subplots(1, 1)\n","  axs.set_title(title or 'Spectrogram (db)')\n","  axs.set_ylabel(ylabel)\n","  axs.set_xlabel('frame')\n","  im = axs.imshow(torchaudio.functional.amplitude_to_DB(spec,10.,1e-10,0), origin='lower', aspect=aspect)\n","  if xmax:\n","    axs.set_xlim((0, xmax))\n","  fig.colorbar(im, ax=axs)\n","  plt.show(block=False)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T02:59:44.509412Z","iopub.status.busy":"2024-05-28T02:59:44.508938Z","iopub.status.idle":"2024-05-28T02:59:44.523360Z","shell.execute_reply":"2024-05-28T02:59:44.522197Z","shell.execute_reply.started":"2024-05-28T02:59:44.509382Z"},"trusted":true},"outputs":[],"source":["train_loader = DataLoader(train_ds,batch_size=256,drop_last=True,shuffle=True)\n","test_loader = DataLoader(test_ds,batch_size=256,shuffle=False,drop_last=False)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T02:59:44.525487Z","iopub.status.busy":"2024-05-28T02:59:44.525063Z","iopub.status.idle":"2024-05-28T02:59:44.539111Z","shell.execute_reply":"2024-05-28T02:59:44.538090Z","shell.execute_reply.started":"2024-05-28T02:59:44.525453Z"},"trusted":true},"outputs":[],"source":["def train(model,epochs,train_loader,test_loader,multi_gpu=None):\n","    global TESTSIZE\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    if multi_gpu is None or multi_gpu is True:\n","        multi_gpu = True if torch.cuda.device_count()>1 else False\n","    else:\n","        multi_gpu = False\n","        print('do not use multi gpu')\n","    optimizer = optim.Adam(model.parameters(),lr=5e-4)\n","    lossfn = nn.CrossEntropyLoss()\n","    acc = -1\n","    not_decrease = 0\n","    if multi_gpu:\n","        model = nn.DataParallel(model)\n","    model.to(device)\n","    for epoch in range(epochs):\n","        if multi_gpu:\n","            model.module.train()\n","        else:\n","            model.train()\n","        batch = 0\n","        for x,y in train_loader:\n","            optimizer.zero_grad()\n","            x = x.to(device)\n","            y = y.to(device)\n","            yHat = model(x,y.squeeze())\n","            loss = lossfn(yHat,y.squeeze()).mean()\n","            loss.backward()\n","            optimizer.step()\n","            print(f\"epoch: {epoch} batch {batch} loss {loss.detach().item()}\")\n","            batch+=1\n","        if multi_gpu:\n","            model.module.eval()\n","        else:\n","            model.eval()\n","        \n","        \n","        with torch.no_grad():\n","            count = 0\n","            for x,y in test_loader:\n","                x = x.to(device)\n","                y = y.to(device)\n","                yHat = model(x)\n","                accuracy = (yHat.argmax(dim=1).long() == y.squeeze()).float().sum().cpu().item()\n","                count += accuracy\n","            accuracy = count/TESTSIZE\n","            print(f\"test accuracy: {accuracy*100}%\")\n","            if accuracy >= acc:\n","                acc = accuracy\n","                not_decrease = 0\n","                if multi_gpu:\n","                    torch.save(model.module.state_dict(),\"vad.pth\")\n","                else:\n","                    torch.save(model.state_dict(),\"vad.pth\")\n","            else:\n","                not_decrease +=1 \n","        if not_decrease == 5:\n","            print('early stopping')\n","            break"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T02:59:44.656156Z","iopub.status.busy":"2024-05-28T02:59:44.655821Z","iopub.status.idle":"2024-05-28T02:59:44.660427Z","shell.execute_reply":"2024-05-28T02:59:44.659351Z","shell.execute_reply.started":"2024-05-28T02:59:44.656107Z"},"trusted":true},"outputs":[],"source":["train(model,50,train_loader,test_loader,False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4613937,"sourceId":8115246,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelInstanceId":47404,"sourceId":56461,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":47404,"sourceId":56794,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
